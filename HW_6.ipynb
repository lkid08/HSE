{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98604cb",
   "metadata": {},
   "source": [
    "Домашку будет легче делать в колабе (убедитесь, что у вас runtype с gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127d1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c422aa0",
   "metadata": {},
   "source": [
    "# Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a72790",
   "metadata": {},
   "source": [
    "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) с помощью tensorflow аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты: \n",
    "1) добавьте лемматизацию в предобработку (любым способом)  \n",
    "2) измените размер окна в большую или меньшую сторону\n",
    "3) измените размерность итоговых векторов\n",
    "\n",
    "Выберете несколько не похожих по смыслу слов (не таких как в семинаре), и протестируйте полученные эмбединги (найдите ближайшие слова и оцените качество, как в семинаре). \n",
    "Постарайтесь обучать модели как можно дольше и на как можно большем количестве данных. (Но если у вас мало времени или ресурсов, то допустимо взять поменьше данных и поставить меньше эпох)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde5fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = open(\"./wiki_data.txt\").read().split('\\n')\n",
    "len(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f71d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = re.sub('#+', ' ', text.lower()).split()\n",
    "    tokens = [token.strip(punctuation) for token in tokens]\n",
    "    normalized_tokens = [token.lower() for token in tokens if token and len(token) < 20]\n",
    "    normalized_tokens = [morph.parse(token)[0].normal_form for token in normalized_tokens]\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "967c1493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258386"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in wiki:\n",
    "    vocab.update(preprocess(text))\n",
    "    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8281d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12442"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab = set()\n",
    "\n",
    "for word in vocab:\n",
    "    if vocab[word] > 30:\n",
    "        filtered_vocab.add(word)\n",
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7101c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {'PAD':0}\n",
    "\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "    \n",
    "id2word = {i:word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d096e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for text in wiki:\n",
    "    tokens = preprocess(text)\n",
    "    if not tokens:\n",
    "        continue\n",
    "    ids = [word2id[token] for token in tokens if token in word2id]\n",
    "    sentences.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6120a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fa0f8",
   "metadata": {},
   "source": [
    "#### skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc89d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_sg(sentences, window=5, batch_size=1000):\n",
    "    left_context_length = (window/2).__ceil__()\n",
    "    right_context_length = window // 2\n",
    "    \n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)-1):\n",
    "                word = sent[i]\n",
    "                context = sent[max(0, i-left_context_length):i] + sent[i+1:i+right_context_length]\n",
    "                for context_word in context:\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(context_word)\n",
    "                    y.append(1)\n",
    "                    \n",
    "                    X_target.append(word)\n",
    "                    X_context.append(np.random.randint(vocab_size))\n",
    "                    y.append(0)\n",
    "                    \n",
    "                    if len(X_target) >= batch_size:\n",
    "                        X_target = np.array(X_target)\n",
    "                        X_context = np.array(X_context)\n",
    "                        y = np.array(y)\n",
    "                        yield ((X_target, X_context), y)\n",
    "                        X_target = []\n",
    "                        X_context = []\n",
    "                        y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc05bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
    "inputs_context = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "\n",
    "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
    "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
    "\n",
    "target = tf.keras.layers.Flatten()(embeddings_target)\n",
    "context = tf.keras.layers.Flatten()(embeddings_context)\n",
    "\n",
    "dot = tf.keras.layers.Dot(1)([target, context])\n",
    "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
    "\n",
    "model_sg = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
    "                       outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_sg.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4963ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 26ms/step - accuracy: 0.7820 - loss: 0.4648 - val_accuracy: 0.7820 - val_loss: 0.5009\n",
      "Epoch 2/4\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 26ms/step - accuracy: 0.8242 - loss: 0.4112 - val_accuracy: 0.8246 - val_loss: 0.3994\n",
      "Epoch 3/4\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 24ms/step - accuracy: 0.8331 - loss: 0.3877 - val_accuracy: 0.8458 - val_loss: 0.3792\n",
      "Epoch 4/4\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 28ms/step - accuracy: 0.8302 - loss: 0.3959 - val_accuracy: 0.8334 - val_loss: 0.3911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f619b503670>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.fit(gen_batches_sg(sentences[:19000], window=8),\n",
    "          validation_data=gen_batches_sg(sentences[19000:], window=8),\n",
    "          batch_size=1000,\n",
    "          steps_per_epoch=10000,\n",
    "          validation_steps=30,\n",
    "         epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697637c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_sg = model_sg.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3dc7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, embeddings):\n",
    "    similar = [id2word[i] for i in \n",
    "               cosine_distances(embeddings[word2id[word]].reshape(1, -1), embeddings).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b81028fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кровать',\n",
       " 'вдруг',\n",
       " 'гомер',\n",
       " 'джерри',\n",
       " 'кирика',\n",
       " 'демон',\n",
       " 'идол',\n",
       " 'прятаться',\n",
       " 'незнакомец',\n",
       " 'вставить']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('кровать', embeddings_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e4ee9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новость',\n",
       " 'цру',\n",
       " 'веб-сайт',\n",
       " 'интернет',\n",
       " 'сми',\n",
       " '«наша',\n",
       " 'петрик',\n",
       " 'прокомментировать',\n",
       " 'network',\n",
       " 'харлин']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('новость', embeddings_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_sg.history.history.keys())\n",
    "plt.plot(model_sg.history.history['accuracy'])\n",
    "plt.plot(model_sg.history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413549d2",
   "metadata": {},
   "source": [
    "#### cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae1ac458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_cbow(sentences, window = 8, batch_size=1000):\n",
    "    left_context_length = (window/2).__ceil__() \n",
    "    right_context_length = window // 2 \n",
    "    \n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)-1):\n",
    "                word = sent[i]\n",
    "                context = sent[max(0, i-left_context_length):i] + sent[i+1:i+right_context_length]\n",
    "\n",
    "                X_target.append(word)\n",
    "                X_context.append(context)\n",
    "                y.append(1)\n",
    "                \n",
    "                X_target.append(np.random.randint(vocab_size))\n",
    "                X_context.append(context)\n",
    "                y.append(0)\n",
    "\n",
    "                if len(X_target) == batch_size:\n",
    "                    X_target = np.array(X_target)\n",
    "                    X_context = tf.keras.preprocessing.sequence.pad_sequences(X_context, maxlen=window)\n",
    "                    y = np.array(y)\n",
    "                    yield ((X_target, X_context), y)\n",
    "                    X_target = []\n",
    "                    X_context = []\n",
    "                    y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ff48bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
    "inputs_context = tf.keras.layers.Input(shape=(8,))\n",
    "\n",
    "\n",
    "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
    "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
    "\n",
    "target = tf.keras.layers.Flatten()(embeddings_target)\n",
    "context = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(embeddings_context)\n",
    "dot = tf.keras.layers.Dot(1)([target, context])\n",
    "\n",
    "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
    "\n",
    "model_cbow = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
    "                       outputs=outputs)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_cbow.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6749d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 28ms/step - accuracy: 0.8176 - loss: 0.4066 - val_accuracy: 0.8779 - val_loss: 0.2978\n",
      "Epoch 2/4\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 27ms/step - accuracy: 0.8842 - loss: 0.2842 - val_accuracy: 0.8965 - val_loss: 0.2562\n",
      "Epoch 3/4\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 26ms/step - accuracy: 0.9098 - loss: 0.2253 - val_accuracy: 0.8973 - val_loss: 0.2601\n",
      "Epoch 4/4\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 27ms/step - accuracy: 0.9204 - loss: 0.1990 - val_accuracy: 0.9058 - val_loss: 0.2372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f616c49cfa0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.fit(gen_batches_cbow(sentences[:19000], window=8),\n",
    "          validation_data=gen_batches_cbow(sentences[19000:], window=8),\n",
    "          batch_size=1000,\n",
    "          steps_per_epoch=5000,\n",
    "          validation_steps=30,\n",
    "         epochs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ec7ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow = model_cbow.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61fbede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кровать',\n",
       " 'прятаться',\n",
       " 'тарелка',\n",
       " 'платье',\n",
       " 'крышка',\n",
       " 'мужик',\n",
       " 'уголок',\n",
       " 'сигарета',\n",
       " 'багажник',\n",
       " 'танцевать']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('кровать', embeddings_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3dcf34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новость',\n",
       " 'bbc',\n",
       " 'сообщить',\n",
       " 'репортаж',\n",
       " 'телеканал',\n",
       " 'повествовать',\n",
       " 'ign',\n",
       " 'тв',\n",
       " 'рассказать',\n",
       " 'youtube']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('новость', embeddings_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b61b7c",
   "metadata": {},
   "source": [
    "# Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff080",
   "metadata": {},
   "source": [
    "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "986c2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85abaea",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5035bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = open('wiki_data.txt').read().split('\\n')\n",
    "texts = [preprocess(text) for text in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76591b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(texts, \n",
    "                             vector_size=250, \n",
    "                             min_count=30, \n",
    "                             max_vocab_size=10000,\n",
    "                             window=8,\n",
    "                             epochs=5,\n",
    "                             sg=1,\n",
    "                             hs=1,\n",
    "                             negative=6,\n",
    "                             sample=1e-4,\n",
    "                             ns_exponent=0.9\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda696ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65e9e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('организация', 0.4628668427467346),\n",
       " ('общественный', 0.45965370535850525),\n",
       " ('деятельность', 0.43834418058395386),\n",
       " ('член', 0.4320813715457916),\n",
       " ('архитектурный', 0.41919445991516113),\n",
       " ('географический', 0.41482317447662354),\n",
       " ('императорский', 0.40922650694847107),\n",
       " ('петербургский', 0.40885287523269653),\n",
       " ('экономический', 0.406019002199173),\n",
       " ('почётный', 0.4043053090572357)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"общество\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aa89f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ни', 0.6312854290008545),\n",
       " ('раз', 0.6022486686706543),\n",
       " ('завоевать', 0.5766454935073853),\n",
       " ('один', 0.5223276615142822),\n",
       " ('медаль', 0.500001847743988),\n",
       " ('летний', 0.49543076753616333),\n",
       " ('страна', 0.492909699678421),\n",
       " ('свой', 0.47197115421295166),\n",
       " ('атланта', 0.47065240144729614),\n",
       " ('принимать', 0.4577661156654358)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"история\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c316089",
   "metadata": {},
   "source": [
    "#### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11ac773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(texts, \n",
    "                             vector_size=250, \n",
    "                             min_count=30, \n",
    "                             max_vocab_size=10000,\n",
    "                             window=8,\n",
    "                             epochs=5,\n",
    "                             sg=1,\n",
    "                             hs=1,\n",
    "                             negative=6,\n",
    "                             sample=1e-4,\n",
    "                             ns_exponent=0.9,\n",
    "                             min_n=4,\n",
    "                             max_n=6\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "568da6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('общественный', 0.6214432120323181),\n",
       " ('сообщество', 0.5312342643737793),\n",
       " ('организация', 0.5034844875335693),\n",
       " ('деятельность', 0.4743858277797699),\n",
       " ('академия', 0.4558217227458954),\n",
       " ('предприниматель', 0.44683998823165894),\n",
       " ('член', 0.4461437463760376),\n",
       " ('экономический', 0.44115740060806274),\n",
       " ('почётный', 0.4265314042568207),\n",
       " ('петербургский', 0.4264426529407501)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar(\"общество\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f17dc1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ни', 0.6313494443893433),\n",
       " ('завоевать', 0.5906491875648499),\n",
       " ('раз', 0.5655894875526428),\n",
       " ('один', 0.51510089635849),\n",
       " ('страна', 0.5041050314903259),\n",
       " ('медаль', 0.4879807233810425),\n",
       " ('принимать', 0.47962844371795654),\n",
       " ('свой', 0.466325968503952),\n",
       " ('участие', 0.4653310477733612),\n",
       " ('летний', 0.45201319456100464)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar(\"история\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb928c",
   "metadata": {},
   "source": [
    "# Задание 3 (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019b0d1",
   "metadata": {},
   "source": [
    "Используя датасет для классификации (labeled.csv) и простую нейронную сеть (последняя модель в семинаре), оцените качество полученных эмбедингов в задании 1 и 2 (4 набора эмбедингов), также проверьте 1 любую из предобученных моделей с rus-vectores (но только не tayga_upos_skipgram_300_2_2019). \n",
    "Какая модель показывает наилучший результат?\n",
    "\n",
    "Убедитесь, что для каждой модели вы корректно воспроизводите пайплайн предобработки (в 1 задании у вас лемматизация, не забудьте ее применить к датасету для классификации; у выбранной предобученной модели может быть своя специфичная предобработка - ее нужно воспроизвести)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed908832",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./labeled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mstrip(punctuation) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m      7\u001b[0m normalized_tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m normalized_tokens \u001b[38;5;241m=\u001b[39m [morph\u001b[38;5;241m.\u001b[39mparse(token)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnormal_form \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m normalized_tokens]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_tokens\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mstrip(punctuation) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m      7\u001b[0m normalized_tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m normalized_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mmorph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnormal_form \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m normalized_tokens]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_tokens\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pymorphy2/analyzer.py:321\u001b[0m, in \u001b[0;36mMorphAnalyzer.parse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_to_parses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pymorphy2/analyzer.py:77\u001b[0m, in \u001b[0;36mProbabilityEstimator.apply_to_parses\u001b[0;34m(self, word, word_lower, parses)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parses:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parses\n\u001b[0;32m---> 77\u001b[0m probs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_t_given_w\u001b[38;5;241m.\u001b[39mprob(word_lower, tag)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (word, tag, normal_form, score, methods_stack) \u001b[38;5;129;01min\u001b[39;00m parses]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(probs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# no P(t|w) information is available; return normalized estimate\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_score_getter, parses))\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pymorphy2/analyzer.py:77\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parses:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parses\n\u001b[0;32m---> 77\u001b[0m probs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_t_given_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (word, tag, normal_form, score, methods_stack) \u001b[38;5;129;01min\u001b[39;00m parses]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(probs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# no P(t|w) information is available; return normalized estimate\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_score_getter, parses))\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/pymorphy2/dawg.py:66\u001b[0m, in \u001b[0;36mConditionalProbDistDAWG.prob\u001b[0;34m(self, word, tag)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprob\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, tag):\n\u001b[1;32m     65\u001b[0m     dawg_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (word, tag)\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdawg_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMULTIPLIER\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/dawg_python/dawgs.py:453\u001b[0m, in \u001b[0;36mIntDAWG.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    452\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 453\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m LOOKUP_ERROR:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/dawg_python/dawgs.py:459\u001b[0m, in \u001b[0;36mIntDAWG.b_get_value\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mb_get_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/dawg_python/wrapper.py:44\u001b[0m, in \u001b[0;36mDictionary.find\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExact matching (returns value)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/dawg_python/wrapper.py:64\u001b[0m, in \u001b[0;36mDictionary.follow_bytes\u001b[0;34m(self, s, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollows transitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[0;32m---> 64\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_from_byte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dolgov/temp/things/.tvenv/lib/python3.10/site-packages/dawg_python/wrapper.py:53\u001b[0m, in \u001b[0;36mDictionary.follow_char\u001b[0;34m(self, label, index)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfollow_char\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, index):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollows a transition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[43munits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_units\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     next_index \u001b[38;5;241m=\u001b[39m (index \u001b[38;5;241m^\u001b[39m offset \u001b[38;5;241m^\u001b[39m label) \u001b[38;5;241m&\u001b[39m units\u001b[38;5;241m.\u001b[39mPRECISION_MASK\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m units\u001b[38;5;241m.\u001b[39mlabel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_units[next_index]) \u001b[38;5;241m!=\u001b[39m label:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./labeled.csv')\n",
    "data['norm_text'] = data.comment.apply(preprocess)\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c18c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in data['norm_text']:\n",
    "    vocab.update(text)\n",
    "    \n",
    "filtered_vocab = set()\n",
    "\n",
    "for word in vocab:\n",
    "    if vocab[word] > 5:\n",
    "        filtered_vocab.add(word)\n",
    "\n",
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = { 'PAD':0}\n",
    "\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "id2word = {i:word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for tokens in data['norm_text']:\n",
    "    ids = [word2id[token] for token in tokens if token in word2id]\n",
    "    X.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=100)\n",
    "y = data.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e44f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(100,))\n",
    "\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=100)(inputs, )\n",
    "mean = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x,  axis=1))(embeddings)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(mean)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fab3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=32,\n",
    "         epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
