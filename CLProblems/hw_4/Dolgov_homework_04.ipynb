{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22014ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: \n",
    "# + language model \n",
    "# + bases guess off 2 preceeding words \n",
    "# + generate 3-5 texts \n",
    "# + calculate perplexity (use 10-50 stored sentences (store at the very beginning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2feab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030d04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding=\"utf-8\").read()\n",
    "news = open('lenta.txt', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf844d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Анимублядский WebM-треддля приличных анимублядей и прочих аутистов. Безграмотное быдло с дубляжом, войсовером, порнографией и котиками, советы мерзких мокрописечников, вниманиебляди всех видов и прочее непотребство отправляется в порнотред <ссылка>.Для поиска сoуса видео сохраняем кадр (правый клик по видео) и ищем его на Для воспроизведения WebM с 10-битным цветом нужно установить плагин vlc ( ) и отключить встроенный в браузер плеер (media. webm. enabled=false в firefox).О кодировании WebMДоступные кодеки — VP8 и VP9 для видео, Vorbis и Opus для звука, максимальный размер файла — 10240КБ, всех файлов в посте — около 40МБ. Делать WebM можно научиться в вики треда: Там находится подробная информация о выборе и настройке кодеков на примерах использования консольных утилит ffmpeg, vpxenc и mkvmerge. Неочевидные моменты— libvorbis при указании битрейта (-b:a) работает в режиме CBR (постоянный битрейт), и это портит качество звука; для режима VBR вместо битрейта надо указывать качество (-'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvach[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede2923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe713f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8709282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c9d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db61e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    for word in normalize(text):\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += (np.log(1/len(norm_dvach)))\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a170e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8c6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7985ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7a4cb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71993, 32250)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_dvach), len(sentences_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9c139",
   "metadata": {},
   "source": [
    "## Set aside sentences for perplexity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d762f839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 71973)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_for_perplexity_dvach = []\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    sens_for_perplexity_dvach.append(sentences_dvach[i])\n",
    "    sentences_dvach.pop(i)\n",
    "\n",
    "len(sens_for_perplexity_dvach), len(sentences_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ed4ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32230)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_for_perplexity_news = []\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    sens_for_perplexity_news.append(sentences_news[i])\n",
    "    sentences_news.pop(i)\n",
    "\n",
    "len(sens_for_perplexity_news), len(sentences_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1f5fe",
   "metadata": {},
   "source": [
    "from nltk.util import ngrams as ngr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence, 2))\n",
    "    trigrams_dvach.update(ngrammer(sentence, 3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence, 2))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9410c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71973"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dvach[\"<start> <start>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae1c8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tri_list = []\n",
    "for trigram, count in trigrams_dvach.items():\n",
    "    if \"<start> <start>\" in trigram:\n",
    "        new_tri_list.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12cc2270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(new_tri_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e38150ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_markov_assumption(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    for ngram in ngrammer(['<start>'] + ['<start>'] + normalize(phrase) + ['<end>'], 2):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9e57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4877f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                         len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {bigram:i for i, bigram in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    matrix_dvach[bigram2id_dvach[word1 + ' ' + word2], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/\n",
    "                                                                                 (bigrams_dvach[word1 + ' ' + word2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a86d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                         len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    matrix_news[bigram2id_news[word1 + ' ' + word2], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                                 (bigrams_news[word1 + ' ' + word2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c13ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        probabilities = matrix[current_idx].toarray()[0]\n",
    "        \n",
    "        probabilities = np.clip(probabilities, 0, None) \n",
    "        probabilities[np.isnan(probabilities)] = 0 \n",
    "        probabilities[np.isinf(probabilities)] = 0 \n",
    "        \n",
    "        probabilities[probabilities == 0] = 1e-10 \n",
    "        probabilities[probabilities < 1e-10] = 1e-10 \n",
    "        probabilities = probabilities.astype(np.float64) \n",
    "        \n",
    "        probabilities /= probabilities.sum()\n",
    "        probabilities[-1] = 1 - probabilities[:-1].sum()\n",
    "      \n",
    "        #chosen = np.random.choice(matrix.shape[1], p=probabilities)\n",
    "        chosen = np.random.default_rng().choice(matrix.shape[1], p=probabilities)\n",
    "        # просто выбирать наиболее вероятное продолжение не получится\n",
    "        # можете попробовать раскоментировать следующую строчку и посмотреть что получается\n",
    "#         chosen = matrix[current_idx].argmax()\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id[start]\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "413cae0a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во текстов по типу двач: 5\n",
      "Пример текста двач:\n",
      "бы подтянул не было же история плагин vlc порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов на втором позёршам а по видео порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников\n"
     ]
    }
   ],
   "source": [
    "text_dvach_list = [generate(matrix_dvach, id2bigram_dvach, bigram2id_dvach).replace('<end>', '\\n') for i in range(5)]\n",
    "print(f'Кол-во текстов по типу двач: {len(text_dvach_list)}\\nПример текста двач:\\n{text_dvach_list[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a456180b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во текстов по типу новостей: 5\n",
      "Пример текста новостей:\n",
      "бы подтянул не было же история плагин vlc порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов на втором позёршам а по видео порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов порнографией и и котиками котиками советы советы мерзких мерзких мокрописечников\n"
     ]
    }
   ],
   "source": [
    "text_news_list = [generate(matrix_news, id2bigram_news, bigram2id_news).replace('<end>', '\\n') for i in range(5)]\n",
    "print(f'Кол-во текстов по типу новостей: {len(text_news_list)}\\nПример текста новостей:\\n{text_dvach_list[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b611599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e30b6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_join_proba_markov_assumption(text, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = text\n",
    "    for ngram in ngrammer(['<start>'] + ['<start>'] + tokens + ['<end>'], 3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' ' + word2\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(bigram_counts[bigram]/trigram_counts[ngram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffab9879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity \n",
    "len(sens_for_perplexity_dvach), len(sens_for_perplexity_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22f7d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_dvach = []\n",
    "for sent in sens_for_perplexity_dvach:\n",
    "    prob, N = compute_join_proba_markov_assumption(sent, bigrams_dvach, trigrams_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps_dvach.append(perplexity(prob, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bfb4ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9039.260746209444"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ps_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94c6eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_news = []\n",
    "for sent in sens_for_perplexity_news:\n",
    "    prob, N = compute_join_proba_markov_assumption(sent, bigrams_news, trigrams_news)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps_news.append(perplexity(prob, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8bc140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45401.24909243016"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ps_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c16d0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "280afd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, id2word, word2id, n=100, max_beams=5, start='<start>'):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    initial_node = Beam(sequence=[start], score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            # наша языковая модель предсказывает на основе предыдущего слова\n",
    "            # достанем его из beam.sequence\n",
    "            #last_id = word2id[beam.sequence[-1]]\n",
    "            last_2_word = beam.sequence[-2:]\n",
    "            last_id = [word2id[word] for word in last_2_word]\n",
    "            \n",
    "            # посмотрим вероятности продолжений для предыдущего слова\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "            \n",
    "            # возьмем топ самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            for top_id in top_idxs:\n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # создадим новый луч на основе текущего и варианта продолжения\n",
    "                new_sequence = beam.sequence + [id2word[top_id]]\n",
    "                # скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = beam.score + np.log1p(probas[top_id])\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "    # в конце возвращаем самый вероятный луч\n",
    "    best_sequence = max(beams, key=lambda x: x.score).sequence\n",
    "\n",
    "    \n",
    "    return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f4b1f",
   "metadata": {},
   "source": [
    "Пустой промт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "145d9db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> а и битрейта котиками максимальный советы доступные мерзких media мокрописечников vlc вниманиебляди 10-битным всех на видов и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов видов и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов видов и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов видов и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов видов и и котиками котиками советы советы мерзких мерзких мокрописечников мокрописечников вниманиебляди вниманиебляди всех всех видов видов и и котиками котиками\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_dvach, word2id_dvach, start='<start>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03f2c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> по в октября напряжения преследовать значительного артиллерию достиг разбили нынешнего <end>\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, word2id_news, start='<start>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd296c78",
   "metadata": {},
   "source": [
    "Промт \"да\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6def4299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "да воспроизводятся воспроизводятся только только указании указании звука звука webm webm его его видео видео уса уса o o по с клик дубляжом правый войсовером кадр порнографией сохраняем и видео котиками уса советы o мерзких что мокрописечников параметром вниманиебляди для всех для видов для и для котиками для советы для мерзких для мокрописечников для вниманиебляди для всех для видов для и для котиками для советы для мерзких для мокрописечников для вниманиебляди для всех для видов для и для котиками для советы для мерзких для мокрописечников для вниманиебляди для всех для видов для и для котиками для советы для мерзких для мокрописечников\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_dvach, word2id_dvach, start='да'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72129ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "да чтобы чтобы объяснены объяснены время время времени времени отмечали отмечали военные военные в в напряжения напряжения значительного значительного достиг достиг нынешнего прошлого милиции пришли всемирную убедить сосредоточенный комплекса сми надеются <end>\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, word2id_news, start='да'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b100a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c32eda",
   "metadata": {},
   "source": [
    "В целом, генерация вышла сравнимой с генерацией из первого задания, \n",
    "и хотя модель все равно выдает рекурсивно повторяющиеся цепочки \n",
    "слов, они не такие длинные, как могут возникать при генерации \n",
    "моделью без лучей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce319ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
